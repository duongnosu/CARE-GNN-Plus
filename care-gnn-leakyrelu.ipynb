{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bffeacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "from operator import itemgetter\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da665d4a",
   "metadata": {},
   "source": [
    "## Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "338a3aed",
   "metadata": {},
   "source": [
    "### Inter Relation Aggregator using GNN\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "40308a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class InterAgg_leaky(nn.Module):\n",
    "\n",
    "\tdef __init__(self, features, feature_dim,\n",
    "\t\t\t\t embed_dim, adj_lists, intraggs,\n",
    "\t\t\t\t inter='GNN', step_size=0.02, cuda=True):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the inter-relation aggregator\n",
    "\t\t:param features: the input node features or embeddings for all nodes\n",
    "\t\t:param feature_dim: the input dimension\n",
    "\t\t:param embed_dim: the output dimension\n",
    "\t\t:param adj_lists: a list of adjacency lists for each single-relation graph\n",
    "\t\t:param intraggs: the intra-relation aggregators used by each single-relation graph\n",
    "\t\t:param inter: the aggregator type: 'Att', 'Weight', 'Mean', 'GNN'\n",
    "\t\t:param step_size: the RL action step size\n",
    "\t\t:param cuda: whether to use GPU\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(InterAgg_leaky, self).__init__()\n",
    "\n",
    "\t\tself.features = features\n",
    "\t\tself.dropout = 0.6\n",
    "\t\tself.adj_lists = adj_lists\n",
    "\t\tself.intra_agg1 = intraggs[0]\n",
    "\t\tself.intra_agg2 = intraggs[1]\n",
    "\t\tself.intra_agg3 = intraggs[2]\n",
    "\t\tself.embed_dim = embed_dim\n",
    "\t\tself.feat_dim = feature_dim\n",
    "\t\tself.inter = inter\n",
    "\t\tself.step_size = step_size\n",
    "\t\tself.cuda = cuda\n",
    "\t\tself.intra_agg1.cuda = cuda\n",
    "\t\tself.intra_agg2.cuda = cuda\n",
    "\t\tself.intra_agg3.cuda = cuda\n",
    "\n",
    "\t\t# RL condition flag\n",
    "\t\tself.RL = True\n",
    "\n",
    "\t\t# number of batches for current epoch, assigned during training\n",
    "\t\tself.batch_num = 0\n",
    "\n",
    "\t\t# initial filtering thresholds\n",
    "\t\tself.thresholds = [0.5, 0.5, 0.5]\n",
    "\n",
    "\t\t# the activation function used by attention mechanism\n",
    "\t\tself.leakyrelu = nn.LeakyReLU(0.2)\n",
    "\n",
    "\t\t# parameter used to transform node embeddings before inter-relation aggregation\n",
    "\t\tself.weight = nn.Parameter(torch.FloatTensor(self.feat_dim, self.embed_dim))\n",
    "\t\tinit.xavier_uniform_(self.weight)\n",
    "\n",
    "\t\t# weight parameter for each relation used by CARE-Weight\n",
    "\t\tself.alpha = nn.Parameter(torch.FloatTensor(self.embed_dim, 3))\n",
    "\t\tinit.xavier_uniform_(self.alpha)\n",
    "\n",
    "\t\t# parameters used by attention layer\n",
    "\t\tself.a = nn.Parameter(torch.FloatTensor(2 * self.embed_dim, 1))\n",
    "\t\tinit.xavier_uniform_(self.a)\n",
    "\n",
    "\t\t# label predictor for similarity measure\n",
    "\t\tself.label_clf = nn.Linear(self.feat_dim, 2)\n",
    "\n",
    "\t\t# initialize the parameter logs\n",
    "\t\tself.weights_log = []\n",
    "\t\tself.thresholds_log = [self.thresholds]\n",
    "\t\tself.relation_score_log = []\n",
    "\n",
    "\tdef forward(self, nodes, labels, train_flag=True):\n",
    "\t\t\"\"\"\n",
    "\t\t:param nodes: a list of batch node ids\n",
    "\t\t:param labels: a list of batch node labels, only used by the RLModule\n",
    "\t\t:param train_flag: indicates whether in training or testing mode\n",
    "\t\t:return combined: the embeddings of a batch of input node features\n",
    "\t\t:return center_scores: the label-aware scores of batch nodes\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# extract 1-hop neighbor ids from adj lists of each single-relation graph\n",
    "\t\tto_neighs = []\n",
    "\t\tfor adj_list in self.adj_lists:\n",
    "\t\t\tto_neighs.append([set(adj_list[int(node)]) for node in nodes])\n",
    "\n",
    "\t\t# find unique nodes and their neighbors used in current batch\n",
    "\t\tunique_nodes = set.union(set.union(*to_neighs[0]), set.union(*to_neighs[1]),\n",
    "\t\t\t\t\t\t\t\t set.union(*to_neighs[2], set(nodes)))\n",
    "\n",
    "\t\t# calculate label-aware scores\n",
    "\t\tif self.cuda:\n",
    "\t\t\tbatch_features = self.features(torch.cuda.LongTensor(list(unique_nodes)))\n",
    "\t\telse:\n",
    "\t\t\tbatch_features = self.features(torch.LongTensor(list(unique_nodes)))\n",
    "\t\tbatch_scores = self.label_clf(batch_features)\n",
    "\t\tid_mapping = {node_id: index for node_id, index in zip(unique_nodes, range(len(unique_nodes)))}\n",
    "\n",
    "\t\t# the label-aware scores for current batch of nodes\n",
    "\t\tcenter_scores = batch_scores[itemgetter(*nodes)(id_mapping), :]\n",
    "\n",
    "\t\t# get neighbor node id list for each batch node and relation\n",
    "\t\tr1_list = [list(to_neigh) for to_neigh in to_neighs[0]]\n",
    "\t\tr2_list = [list(to_neigh) for to_neigh in to_neighs[1]]\n",
    "\t\tr3_list = [list(to_neigh) for to_neigh in to_neighs[2]]\n",
    "\n",
    "\t\t# assign label-aware scores to neighbor nodes for each batch node and relation\n",
    "\t\tr1_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r1_list]\n",
    "\t\tr2_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r2_list]\n",
    "\t\tr3_scores = [batch_scores[itemgetter(*to_neigh)(id_mapping), :].view(-1, 2) for to_neigh in r3_list]\n",
    "\n",
    "\t\t# count the number of neighbors kept for aggregation for each batch node and relation\n",
    "\t\tr1_sample_num_list = [math.ceil(len(neighs) * self.thresholds[0]) for neighs in r1_list]\n",
    "\t\tr2_sample_num_list = [math.ceil(len(neighs) * self.thresholds[1]) for neighs in r2_list]\n",
    "\t\tr3_sample_num_list = [math.ceil(len(neighs) * self.thresholds[2]) for neighs in r3_list]\n",
    "\n",
    "\t\t# intra-aggregation steps for each relation\n",
    "\t\t# Eq. (8) in the paper\n",
    "\t\tr1_feats, r1_scores = self.intra_agg1.forward(nodes, r1_list, center_scores, r1_scores, r1_sample_num_list)\n",
    "\t\tr2_feats, r2_scores = self.intra_agg2.forward(nodes, r2_list, center_scores, r2_scores, r2_sample_num_list)\n",
    "\t\tr3_feats, r3_scores = self.intra_agg3.forward(nodes, r3_list, center_scores, r3_scores, r3_sample_num_list)\n",
    "\n",
    "\t\t# concat the intra-aggregated embeddings from each relation\n",
    "\t\tneigh_feats = torch.cat((r1_feats, r2_feats, r3_feats), dim=0)\n",
    "\n",
    "\t\t# get features or embeddings for batch nodes\n",
    "\t\tif self.cuda and isinstance(nodes, list):\n",
    "\t\t\tindex = torch.LongTensor(nodes).cuda()\n",
    "\t\telse:\n",
    "\t\t\tindex = torch.LongTensor(nodes)\n",
    "\t\tself_feats = self.features(index)\n",
    "\n",
    "\t\t# number of nodes in a batch\n",
    "\t\tn = len(nodes)\n",
    "\n",
    "\t\t# inter-relation aggregation steps\n",
    "\t\t# Eq. (9) in the paper\n",
    "\t\t# if self.inter == 'Att':\n",
    "\t\t# \t# 1) CARE-Att Inter-relation Aggregator\n",
    "\t\t# \tcombined, attention = att_inter_agg(len(self.adj_lists), self.leakyrelu, self_feats, neigh_feats, self.embed_dim,\n",
    "\t\t# \t\t\t\t\t\t\t\t\t\tself.weight, self.a, n, self.dropout, self.training, self.cuda)\n",
    "\t\t# elif self.inter == 'Weight':\n",
    "\t\t# \t# 2) CARE-Weight Inter-relation Aggregator\n",
    "\t\t# \tcombined = weight_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, self.alpha, n, self.cuda)\n",
    "\t\t# \tgem_weights = F.softmax(torch.sum(self.alpha, dim=0), dim=0).tolist()\n",
    "\t\t# \tif train_flag:\n",
    "\t\t# \t\tprint(f'Weights: {gem_weights}')\n",
    "\t\t# elif self.inter == 'Mean':\n",
    "\t\t# \t# 3) CARE-Mean Inter-relation Aggregator\n",
    "\t\t# \tcombined = mean_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, n, self.cuda)\n",
    "\t\tif self.inter == 'GNN':\n",
    "\t\t\t# 4) CARE-GNN Inter-relation Aggregator\n",
    "\t\t\tcombined = threshold_inter_agg(len(self.adj_lists), self_feats, neigh_feats, self.embed_dim, self.weight, self.thresholds, n, self.cuda)\n",
    "\n",
    "\t\t# the reinforcement learning module\n",
    "\t\tif self.RL and train_flag:\n",
    "\t\t\trelation_scores, rewards, thresholds, stop_flag = RLModule([r1_scores, r2_scores, r3_scores],\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   self.relation_score_log, labels, self.thresholds,\n",
    "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t   self.batch_num, self.step_size)\n",
    "\t\t\tself.thresholds = thresholds\n",
    "\t\t\tself.RL = stop_flag\n",
    "\t\t\tself.relation_score_log.append(relation_scores)\n",
    "\t\t\tself.thresholds_log.append(self.thresholds)\n",
    "\n",
    "\t\treturn combined, center_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163bdaec",
   "metadata": {},
   "source": [
    "### Instra Relation Aggregator with LeakyReLU (Proposed)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7ad82239",
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntraAgg_leaky(nn.Module):\n",
    "\n",
    "\tdef __init__(self, features, feat_dim, cuda=False):\n",
    "\t\t\"\"\"\n",
    "\t\tInitialize the intra-relation aggregator\n",
    "\t\t:param features: the input node features or embeddings for all nodes\n",
    "\t\t:param feat_dim: the input dimension\n",
    "\t\t:param cuda: whether to use GPU\n",
    "\t\t\"\"\"\n",
    "\t\tsuper(IntraAgg_leaky, self).__init__()\n",
    "\n",
    "\t\tself.features = features\n",
    "\t\tself.cuda = cuda\n",
    "\t\tself.feat_dim = feat_dim\n",
    "\n",
    "\tdef forward(self, nodes, to_neighs_list, batch_scores, neigh_scores, sample_list):\n",
    "\t\t\"\"\"\n",
    "\t\tCode partially from https://github.com/williamleif/graphsage-simple/\n",
    "\t\t:param nodes: list of nodes in a batch\n",
    "\t\t:param to_neighs_list: neighbor node id list for each batch node in one relation\n",
    "\t\t:param batch_scores: the label-aware scores of batch nodes\n",
    "\t\t:param neigh_scores: the label-aware scores 1-hop neighbors each batch node in one relation\n",
    "\t\t:param sample_list: the number of neighbors kept for each batch node in one relation\n",
    "\t\t:return to_feats: the aggregated embeddings of batch nodes neighbors in one relation\n",
    "\t\t:return samp_scores: the average neighbor distances for each relation after filtering\n",
    "\t\t\"\"\"\n",
    "\n",
    "\t\t# filer neighbors under given relation\n",
    "\t\tsamp_neighs, samp_scores = filter_neighs_ada_threshold(batch_scores, neigh_scores, to_neighs_list, sample_list)\n",
    "\n",
    "\t\t# find the unique nodes among batch nodes and the filtered neighbors\n",
    "\t\tunique_nodes_list = list(set.union(*samp_neighs))\n",
    "\t\tunique_nodes = {n: i for i, n in enumerate(unique_nodes_list)}\n",
    "\n",
    "\t\t# intra-relation aggregation only with sampled neighbors\n",
    "\t\tmask = Variable(torch.zeros(len(samp_neighs), len(unique_nodes)))\n",
    "\t\tcolumn_indices = [unique_nodes[n] for samp_neigh in samp_neighs for n in samp_neigh]\n",
    "\t\trow_indices = [i for i in range(len(samp_neighs)) for _ in range(len(samp_neighs[i]))]\n",
    "\t\tmask[row_indices, column_indices] = 1\n",
    "\t\tif self.cuda:\n",
    "\t\t\tmask = mask.cuda()\n",
    "\t\tnum_neigh = mask.sum(1, keepdim=True)\n",
    "\t\tmask = mask.div(num_neigh)\n",
    "\t\tif self.cuda:\n",
    "\t\t\tembed_matrix = self.features(torch.LongTensor(unique_nodes_list).cuda())\n",
    "\t\telse:\n",
    "\t\t\tembed_matrix = self.features(torch.LongTensor(unique_nodes_list))\n",
    "\t\tto_feats = mask.mm(embed_matrix)\n",
    "\t\t# to_feats = F.relu(to_feats)\n",
    "\t\tto_feats = nn.LeakyReLU(0.2)(to_feats) # Proposed Leakly Relu Activtion eqn 8\n",
    "\t\treturn to_feats, samp_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dc8af5",
   "metadata": {},
   "source": [
    "### Reinforcement Learning Module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7294e1e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def RLModule(scores, scores_log, labels, thresholds, batch_num, step_size):\n",
    "\t\"\"\"\n",
    "\tThe reinforcement learning module.\n",
    "\tIt updates the neighbor filtering threshold for each relation based\n",
    "\ton the average neighbor distances between two consecutive epochs.\n",
    "\t:param scores: the neighbor nodes label-aware scores for each relation\n",
    "\t:param scores_log: a list stores the relation average distances for each batch\n",
    "\t:param labels: the batch node labels used to select positive nodes\n",
    "\t:param thresholds: the current neighbor filtering thresholds for each relation\n",
    "\t:param batch_num: numbers batches in an epoch\n",
    "\t:param step_size: the RL action step size\n",
    "\t:return relation_scores: the relation average distances for current batch\n",
    "\t:return rewards: the reward for given thresholds in current epoch\n",
    "\t:return new_thresholds: the new filtering thresholds updated according to the rewards\n",
    "\t:return stop_flag: the RL terminal condition flag\n",
    "\t\"\"\"\n",
    "\n",
    "\trelation_scores = []\n",
    "\tstop_flag = True\n",
    "\n",
    "\t# only compute the average neighbor distances for positive nodes\n",
    "\tpos_index = (labels == 1).nonzero().tolist()\n",
    "\tpos_index = [i[0] for i in pos_index]\n",
    "\n",
    "\t# compute average neighbor distances for each relation\n",
    "\tfor score in scores:\n",
    "\t\tpos_scores = itemgetter(*pos_index)(score)\n",
    "\t\tneigh_count = sum([1 if isinstance(i, float) else len(i) for i in pos_scores])\n",
    "\t\tpos_sum = [i if isinstance(i, float) else sum(i) for i in pos_scores]\n",
    "\t\trelation_scores.append(sum(pos_sum) / neigh_count)\n",
    "\n",
    "\tif len(scores_log) % batch_num != 0 or len(scores_log) < 2 * batch_num:\n",
    "\t\t# do not call RL module within the epoch or within the first two epochs\n",
    "\t\trewards = [0, 0, 0]\n",
    "\t\tnew_thresholds = thresholds\n",
    "\telse:\n",
    "\t\t# update thresholds according to average scores in last epoch\n",
    "\t\t# Eq.(5) in the paper\n",
    "\t\tprevious_epoch_scores = [sum(s) / batch_num for s in zip(*scores_log[-2 * batch_num:-batch_num])]\n",
    "\t\tcurrent_epoch_scores = [sum(s) / batch_num for s in zip(*scores_log[-batch_num:])]\n",
    "\n",
    "\t\t# compute reward for each relation and update the thresholds according to reward\n",
    "\t\t# Eq. (6) in the paper\n",
    "\t\trewards = [1 if previous_epoch_scores[i] - s >= 0 else -1 for i, s in enumerate(current_epoch_scores)]\n",
    "\t\tnew_thresholds = [thresholds[i] + step_size if r == 1 else thresholds[i] - step_size for i, r in enumerate(rewards)]\n",
    "\n",
    "\t\t# avoid overflow\n",
    "\t\tnew_thresholds = [0.999 if i > 1 else i for i in new_thresholds]\n",
    "\t\tnew_thresholds = [0.001 if i < 0 else i for i in new_thresholds]\n",
    "\n",
    "\t\tprint(f'epoch scores: {current_epoch_scores}')\n",
    "\t\tprint(f'rewards: {rewards}')\n",
    "\t\tprint(f'thresholds: {new_thresholds}')\n",
    "\n",
    "\treturn relation_scores, rewards, new_thresholds, stop_flag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "665f612b",
   "metadata": {},
   "source": [
    "### Filter neighbors from label predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "36cba04d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_neighs_ada_threshold(center_scores, neigh_scores, neighs_list, sample_list):\n",
    "\t\"\"\"\n",
    "\tFilter neighbors according label predictor result with adaptive thresholds\n",
    "\t:param center_scores: the label-aware scores of batch nodes\n",
    "\t:param neigh_scores: the label-aware scores 1-hop neighbors each batch node in one relation\n",
    "\t:param neighs_list: neighbor node id list for each batch node in one relation\n",
    "\t:param sample_list: the number of neighbors kept for each batch node in one relation\n",
    "\t:return samp_neighs: the neighbor indices and neighbor simi scores\n",
    "\t:return samp_scores: the average neighbor distances for each relation after filtering\n",
    "\t\"\"\"\n",
    "\n",
    "\tsamp_neighs = []\n",
    "\tsamp_scores = []\n",
    "\tfor idx, center_score in enumerate(center_scores):\n",
    "\t\tcenter_score = center_scores[idx][0]\n",
    "\t\tneigh_score = neigh_scores[idx][:, 0].view(-1, 1)\n",
    "\t\tcenter_score = center_score.repeat(neigh_score.size()[0], 1)\n",
    "\t\tneighs_indices = neighs_list[idx]\n",
    "\t\tnum_sample = sample_list[idx]\n",
    "\n",
    "\t\t# compute the L1-distance of batch nodes and their neighbors\n",
    "\t\t# Eq. (2) in paper\n",
    "\t\tscore_diff = torch.abs(center_score - neigh_score).squeeze()\n",
    "\t\tsorted_scores, sorted_indices = torch.sort(score_diff, dim=0, descending=False)\n",
    "\t\tselected_indices = sorted_indices.tolist()\n",
    "\n",
    "\t\t# top-p sampling according to distance ranking and thresholds\n",
    "\t\t# Section 3.3.1 in paper\n",
    "\t\tif len(neigh_scores[idx]) > num_sample + 1:\n",
    "\t\t\tselected_neighs = [neighs_indices[n] for n in selected_indices[:num_sample]]\n",
    "\t\t\tselected_scores = sorted_scores.tolist()[:num_sample]\n",
    "\t\telse:\n",
    "\t\t\tselected_neighs = neighs_indices\n",
    "\t\t\tselected_scores = score_diff.tolist()\n",
    "\t\t\tif isinstance(selected_scores, float):\n",
    "\t\t\t\tselected_scores = [selected_scores]\n",
    "\n",
    "\t\tsamp_neighs.append(set(selected_neighs))\n",
    "\t\tsamp_scores.append(selected_scores)\n",
    "\n",
    "\treturn samp_neighs, samp_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddba3397",
   "metadata": {},
   "source": [
    "### CARE-GNN inter-relation aggregator using LeakyReLU (Proposed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7aeb8e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def threshold_inter_agg(num_relations, self_feats, neigh_feats, embed_dim, weight, threshold, n, cuda):\n",
    "\t\"\"\"\n",
    "\tCARE-GNN inter-relation aggregator\n",
    "\tEq. (9) in the paper use Leaky Relu instread of Relu\n",
    "\t:param num_relations: number of relations in the graph\n",
    "\t:param self_feats: batch nodes features or embeddings\n",
    "\t:param neigh_feats: intra-relation aggregated neighbor embeddings for each relation\n",
    "\t:param embed_dim: the dimension of output embedding\n",
    "\t:param weight: parameter used to transform node embeddings before inter-relation aggregation\n",
    "\t:param threshold: the neighbor filtering thresholds used as aggregating weights\n",
    "\t:param n: number of nodes in a batch\n",
    "\t:param cuda: whether use GPU\n",
    "\t:return: inter-relation aggregated node embeddings\n",
    "\t\"\"\"\n",
    "\n",
    "\t# transform batch node embedding and neighbor embedding in each relation with weight parameter\n",
    "\tcenter_h = torch.mm(self_feats, weight)\n",
    "\tneigh_h = torch.mm(neigh_feats, weight)\n",
    "\n",
    "\t# initialize the final neighbor embedding\n",
    "\tif cuda:\n",
    "\t\taggregated = torch.zeros(size=(n, embed_dim)).cuda()\n",
    "\telse:\n",
    "\t\taggregated = torch.zeros(size=(n, embed_dim))\n",
    "\n",
    "\t# add weighted neighbor embeddings in each relation together\n",
    "\tfor r in range(num_relations):\n",
    "\t\taggregated += neigh_h[r * n:(r + 1) * n, :] * threshold[r]\n",
    "\n",
    "\t# sum aggregated neighbor embedding and batch node embedding\n",
    "\t# feed them to activation function\n",
    "\t# combined = F.relu(center_h + aggregated)\n",
    "\tcombined = nn.LeakyReLU(0.2)(center_h + aggregated) # Proposed Leakly Relu Activtion eqn 9\n",
    "\n",
    "\treturn combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfe2edc6",
   "metadata": {},
   "source": [
    "# Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dcaa829",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLayerCARE_Leaky(nn.Module):\n",
    "    \"\"\"\n",
    "    Multi-Layer CARE-GNN- with additional transform layers and residual connections between embeddings\n",
    "    \"\"\"\n",
    "    def __init__(self, num_classes, features, feat_dim, embed_dim, adj_lists,\n",
    "                 num_layers=2, lambda_1=2.0, step_size=0.02, cuda=False):\n",
    "        super(MultiLayerCARE_Leaky, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.lambda_1 = lambda_1\n",
    "        self.xent = nn.CrossEntropyLoss()\n",
    "\n",
    "        # Core CARE-GNN components\n",
    "        intra1 = IntraAgg_leaky(features, feat_dim, cuda=cuda)\n",
    "        intra2 = IntraAgg_leaky(features, feat_dim, cuda=cuda)\n",
    "        intra3 = IntraAgg_leaky(features, feat_dim, cuda=cuda)\n",
    "\n",
    "        self.inter1 = InterAgg_leaky(features, feat_dim, embed_dim, adj_lists,\n",
    "                              [intra1, intra2, intra3],\n",
    "                              inter='GNN', step_size=step_size, cuda=cuda)\n",
    "\n",
    "        # Disable RL to prevent modulo by zero error\n",
    "        self.inter1.RL = False\n",
    "\n",
    "        ##### NOVELTY PROPOSED #####\n",
    "        # Novelty: Add additional transformation layers\n",
    "        \n",
    "        self.transforms = nn.ModuleList()\n",
    "        for i in range(num_layers - 1):\n",
    "            self.transforms.append(nn.Linear(embed_dim, embed_dim))\n",
    "        #############################\n",
    "        # Final classifier\n",
    "        self.weight = nn.Parameter(torch.FloatTensor(embed_dim, num_classes))\n",
    "        init.xavier_uniform_(self.weight)\n",
    "\n",
    "    def forward(self, nodes, labels, train_flag=True):\n",
    "        # First layer (CARE-GNN without RL issues)\n",
    "        embeddings, label_scores = self.inter1(nodes, labels, train_flag)\n",
    "\n",
    "        # NOVELTY: Apply additional transformations\n",
    "        #applies a sequence of linear layers with ReLU, followed by residual connections (like in ResNet):\n",
    "        prev_embeddings = embeddings\n",
    "        for i, transform in enumerate(self.transforms):\n",
    "            embeddings = F.relu(transform(embeddings))\n",
    "            # ALSOAdd residual connection for deeper layers\n",
    "            if i > 0:\n",
    "                embeddings = embeddings + prev_embeddings\n",
    "            prev_embeddings = embeddings\n",
    "        ##################################################\n",
    "        # Final prediction\n",
    "        scores = torch.mm(embeddings, self.weight)\n",
    "        return scores, label_scores\n",
    "\n",
    "    def to_prob(self, nodes, labels, train_flag=True):\n",
    "        gnn_scores, label_scores = self.forward(nodes, labels, train_flag)\n",
    "        gnn_prob = nn.functional.softmax(gnn_scores, dim=1)\n",
    "        label_prob = nn.functional.softmax(label_scores, dim=1)\n",
    "        return gnn_prob, label_prob\n",
    "\n",
    "    def loss(self, nodes, labels, train_flag=True):\n",
    "        gnn_scores, label_scores = self.forward(nodes, labels, train_flag)\n",
    "\n",
    "        # GNN loss\n",
    "        gnn_loss = self.xent(gnn_scores, labels.squeeze())\n",
    "\n",
    "        # Similarity loss (same as baseline)\n",
    "        label_loss = self.xent(label_scores, labels.squeeze())\n",
    "\n",
    "        # Combined loss\n",
    "        final_loss = gnn_loss + self.lambda_1 * label_loss\n",
    "        return final_loss\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c43df4bf",
   "metadata": {},
   "source": [
    "# Experiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52f3ff75",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bfd8457e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from scipy.io import loadmat\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import networkx as nx\n",
    "import random\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from torch.nn import init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from operator import itemgetter\n",
    "import math\n",
    "\n",
    "# Import your existing modules\n",
    "from model import OneLayerCARE\n",
    "from layers import IntraAgg, InterAgg\n",
    "from utils import normalize, pos_neg_split, undersample, test_care\n",
    "\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "random.seed(42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d8026faf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.24.4'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae230ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Amazon Fraud Detection Dataset ===\")\n",
    "\n",
    "# Load Amazon.mat file directly\n",
    "data_file = loadmat('data/Amazon.mat')\n",
    "labels = data_file['label'].flatten()\n",
    "feat_data = data_file['features'].todense().A\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcf1be95",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Load the preprocessed adjacency lists\n",
    "with open('data/amz_upu_adjlists.pickle', 'rb') as f:\n",
    "    relation1 = pickle.load(f)  # User-Product-User\n",
    "with open('data/amz_usu_adjlists.pickle', 'rb') as f:\n",
    "    relation2 = pickle.load(f)  # User-Service-User\n",
    "with open('data/amz_uvu_adjlists.pickle', 'rb') as f:\n",
    "    relation3 = pickle.load(f)  # User-View-User\n",
    "\n",
    "# Create homogeneous adjacency list (combine all relations)\n",
    "homo = data_file['homo']\n",
    "\n",
    "print(f\"Dataset Statistics:\")\n",
    "print(f\"Total nodes: {len(labels)}\")\n",
    "print(f\"Fraudulent users: {np.sum(labels == 1)} ({np.mean(labels==1)*100:.2f}%)\")\n",
    "print(f\"Feature dimensions: {feat_data.shape}\")\n",
    "\n",
    "print(f\"\\nRelation Statistics:\")\n",
    "print(f\"User-Product-User nodes: {len(relation1)}\")\n",
    "print(f\"User-Service-User nodes: {len(relation2)}\")\n",
    "print(f\"User-View-User nodes: {len(relation3)}\")\n",
    "\n",
    "# ADD THIS BLOCK HERE (moved from Section 2):\n",
    "print(f\"\\n=== Feature Preprocessing ===\")\n",
    "# Prepare features and adjacency lists\n",
    "feat_data_normalized = normalize(feat_data)\n",
    "features = nn.Embedding(feat_data.shape[0], feat_data.shape[1])\n",
    "features.weight = nn.Parameter(torch.FloatTensor(feat_data_normalized), requires_grad=False)\n",
    "\n",
    "adj_lists = [relation1, relation2, relation3]\n",
    "print(\"✅ Features normalized and prepared\")\n",
    "print(\"✅ Adjacency lists organized\")\n",
    "\n",
    "# # Visualize class distribution\n",
    "# plt.figure(figsize=(8, 4))\n",
    "# plt.bar(['Legitimate', 'Fraudulent'], [np.sum(labels == 0), np.sum(labels == 1)])\n",
    "# plt.title('Class Distribution in Amazon Dataset')\n",
    "# plt.ylabel('Number of Users')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02d918a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2787f17b",
   "metadata": {},
   "source": [
    "## Create Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6648901",
   "metadata": {},
   "source": [
    "### CARE-GNN Base Model ( paper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789844d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Model hyperparameters\n",
    "embed_dim = 64\n",
    "step_size = 0.02\n",
    "lambda_1 = 2.0\n",
    "batch_size = 256\n",
    "\n",
    "print(f\"Model Configuration:\")\n",
    "print(f\"- Embedding dimension: {embed_dim}\")\n",
    "print(f\"- RL step size: {step_size}\")\n",
    "print(f\"- Similarity loss weight: {lambda_1}\")\n",
    "print(f\"- Batch size: {batch_size}\")\n",
    "\n",
    "# Build CARE-GNN model (features and adj_lists already prepared in Section 1)\n",
    "intra1 = IntraAgg(features, feat_data.shape[1], cuda=False)\n",
    "intra2 = IntraAgg(features, feat_data.shape[1], cuda=False)\n",
    "intra3 = IntraAgg(features, feat_data.shape[1], cuda=False)\n",
    "\n",
    "inter = InterAgg(features, feat_data.shape[1], embed_dim, adj_lists,\n",
    "                 [intra1, intra2, intra3], inter='GNN', step_size=step_size, cuda=False)\n",
    "\n",
    "model = OneLayerCARE(num_classes=2, inter1=inter, lambda_1=lambda_1)\n",
    "\n",
    "print(f\"✅ CARE-GNN base model built successfully!\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7deea01e",
   "metadata": {},
   "source": [
    "### Proposed Multi-Layer CARE-GNN with LeakyReLU and residual connections between embedding layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a803f2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bdbe376",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model2_leaky = MultiLayerCARE_Leaky(\n",
    "    num_classes=2,\n",
    "    features=features,\n",
    "    feat_dim=feat_data.shape[1],\n",
    "    embed_dim=embed_dim,\n",
    "    adj_lists=adj_lists,\n",
    "    num_layers=2,\n",
    "    lambda_1=lambda_1,\n",
    "    step_size=step_size,\n",
    "    cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce79e255",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model2_leaky))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc7e4cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "model3_leaky = MultiLayerCARE_Leaky(\n",
    "    num_classes=2,\n",
    "    features=features,\n",
    "    feat_dim=feat_data.shape[1],\n",
    "    embed_dim=embed_dim,\n",
    "    adj_lists=adj_lists,\n",
    "    num_layers=3, # 3 layers\n",
    "    lambda_1=lambda_1,\n",
    "    step_size=step_size,\n",
    "    cuda=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2f6330",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(model3_leaky))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e41fb4d",
   "metadata": {},
   "source": [
    "# Ultils Functions for training and evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179acdb",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "439a218f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(\"=== Training Configuration ===\")\n",
    "\n",
    "# Create train/test split for Amazon dataset\n",
    "# Amazon: first 3305 nodes are unlabeled\n",
    "labeled_indices = list(range(3305, len(labels)))\n",
    "labeled_labels = labels[3305:]\n",
    "\n",
    "idx_train, idx_test, y_train, y_test = train_test_split(\n",
    "    labeled_indices, labeled_labels,\n",
    "    stratify=labeled_labels,\n",
    "    test_size=0.6,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(f\"Training samples: {len(idx_train)}\")\n",
    "print(f\"Test samples: {len(idx_test)}\")\n",
    "\n",
    "# Split positive and negative samples for undersampling\n",
    "train_pos, train_neg = pos_neg_split(idx_train, y_train)\n",
    "print(f\"Training - Positive: {len(train_pos)}, Negative: {len(train_neg)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803a5b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_care_model(model, model_name, train_pos, train_neg, labels, \n",
    "                      learning_rate=0.01, weight_decay=1e-3, momentum=0.9, num_epochs=15, \n",
    "                      batch_size=256, scale=1, verbose=True, plot_loss=True):\n",
    "    \"\"\"\n",
    "    General training function for CARE-GNN models (including multi-layer variants)\n",
    "    \n",
    "    Args:\n",
    "        model: The CARE model to train (model2_leaky, model3_leaky, etc.)\n",
    "        model_name: String name for the model (for logging and plotting)\n",
    "        train_pos: List of positive training sample indices\n",
    "        train_neg: List of negative training sample indices\n",
    "        labels: Array of all node labels\n",
    "        learning_rate: Learning rate for SGD optimizer\n",
    "        weight_decay: Weight decay for SGD optimizer\n",
    "        momentum: Momentum for SGD optimizer\n",
    "        num_epochs: Number of training epochs\n",
    "        batch_size: Batch size for training\n",
    "        scale: Scale factor for undersampling (1 means balanced)\n",
    "        verbose: Whether to print training progress\n",
    "        plot_loss: Whether to plot training loss curve\n",
    "    \n",
    "    Returns:\n",
    "        train_losses: List of training losses per epoch\n",
    "        trained_model: The trained model\n",
    "    \"\"\"\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"=== Training {model_name} ===\")\n",
    "    \n",
    "    # Setup SGD optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate, \n",
    "                               weight_decay=weight_decay, momentum=momentum)\n",
    "    \n",
    "    # Track losses\n",
    "    train_losses = []\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        \n",
    "        # Undersample negative samples for balanced training\n",
    "        sampled_idx_train = undersample(train_pos, train_neg, scale=scale)\n",
    "        random.shuffle(sampled_idx_train)\n",
    "        \n",
    "        # Calculate number of batches\n",
    "        num_batches = (len(sampled_idx_train) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Set batch number for RL module (if model has inter1 with RL)\n",
    "        if hasattr(model, 'inter1') and hasattr(model.inter1, 'batch_num'):\n",
    "            model.inter1.batch_num = num_batches\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        \n",
    "        # Batch training\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min((batch_idx + 1) * batch_size, len(sampled_idx_train))\n",
    "            batch_nodes = sampled_idx_train[start_idx:end_idx]\n",
    "            batch_labels = torch.LongTensor(labels[batch_nodes])\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            loss = model.loss(batch_nodes, batch_labels, train_flag=True)\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "        \n",
    "        # Calculate average loss for epoch\n",
    "        avg_loss = epoch_loss / num_batches\n",
    "        train_losses.append(avg_loss)\n",
    "        \n",
    "        # Print progress\n",
    "        if verbose and epoch % 3 == 0:\n",
    "            print(f\"Epoch {epoch:2d}: Loss = {avg_loss:.4f}\")\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"✅ {model_name} training completed!\")\n",
    "    \n",
    "    # Plot training loss\n",
    "    if plot_loss:\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(train_losses, label=model_name)\n",
    "        plt.title(f'{model_name} Training Loss')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.show()\n",
    "    \n",
    "    return train_losses, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e26b8fbc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "35f60d5a",
   "metadata": {},
   "source": [
    "### TRAINING BASE CARE-GNN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab0d996",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train 2-layer model with SGD\n",
    "losses_2layer, trained_model2 = train_care_model(\n",
    "    model=model2_leaky,\n",
    "    model_name=\"Multi-Layer CARE (2 layers + LeakyReLU)\",\n",
    "    train_pos=train_pos,\n",
    "    train_neg=train_neg,\n",
    "    labels=labels,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=1e-3,\n",
    "    momentum=0.9,        # SGD momentum\n",
    "    num_epochs=15,\n",
    "    batch_size=256\n",
    ")\n",
    "\n",
    "# Train 3-layer model with SGD\n",
    "losses_3layer, trained_model3 = train_care_model(\n",
    "    model=model3_leaky,\n",
    "    model_name=\"Multi-Layer CARE (3 layers + LeakyReLU)\",\n",
    "    train_pos=train_pos,\n",
    "    train_neg=train_neg,\n",
    "    labels=labels,\n",
    "    learning_rate=0.01,\n",
    "    weight_decay=1e-3,\n",
    "    momentum=0.9,        # SGD momentum\n",
    "    num_epochs=15,\n",
    "    batch_size=256\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f5d040c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Training loss comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(losses_2layer, label='2-Layer CARE + LeakyReLU', linewidth=2, color='blue')\n",
    "plt.plot(losses_3layer, label='3-Layer CARE + LeakyReLU', linewidth=2, color='red')\n",
    "plt.title('Training Loss Comparison (SGD)')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Final loss comparison bar chart\n",
    "plt.subplot(1, 2, 2)\n",
    "models = ['2-Layer', '3-Layer']\n",
    "final_losses = [losses_2layer[-1], losses_3layer[-1]]\n",
    "colors = ['blue', 'red']\n",
    "plt.bar(models, final_losses, color=colors, alpha=0.7)\n",
    "plt.title('Final Training Loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for i, v in enumerate(final_losses):\n",
    "    plt.text(i, v + 0.001, f'{v:.4f}', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c36b2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate 2-layer model\n",
    "print(\"\\n=== 2-Layer CARE Model Evaluation ===\")\n",
    "gnn_auc_2l, label_auc_2l, gnn_recall_2l, label_recall_2l = test_care(idx_test, y_test, trained_model2, batch_size)\n",
    "\n",
    "print(f\"2-Layer Model Results:\")\n",
    "print(f\"  AUC:    {gnn_auc_2l:.4f}\")\n",
    "print(f\"  Recall: {gnn_recall_2l:.4f}\")\n",
    "\n",
    "# Evaluate 3-layer model  \n",
    "print(\"\\n=== 3-Layer CARE Model Evaluation ===\")\n",
    "gnn_auc_3l, label_auc_3l, gnn_recall_3l, label_recall_3l = test_care(idx_test, y_test, trained_model3, batch_size)\n",
    "\n",
    "print(f\"3-Layer Model Results:\")\n",
    "print(f\"  AUC:    {gnn_auc_3l:.4f}\")\n",
    "print(f\"  Recall: {gnn_recall_3l:.4f}\")\n",
    "\n",
    "# Manual comparison plot\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# AUC comparison\n",
    "plt.subplot(1, 2, 1)\n",
    "models = ['2-Layer', '3-Layer']\n",
    "aucs = [gnn_auc_2l, gnn_auc_3l]\n",
    "plt.bar(models, aucs, color=['skyblue', 'lightcoral'])\n",
    "plt.title('AUC Comparison')\n",
    "plt.ylabel('AUC Score')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "# Recall comparison\n",
    "plt.subplot(1, 2, 2)\n",
    "recalls = [gnn_recall_2l, gnn_recall_3l]\n",
    "plt.bar(models, recalls, color=['orange', 'lightgreen'])\n",
    "plt.title('Recall Comparison')\n",
    "plt.ylabel('Recall Score')\n",
    "plt.ylim([0, 1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "CVAE",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
